{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-05T16:19:14.533788Z",
     "start_time": "2023-12-05T16:19:02.364320Z"
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "rwth_phoenix = datasets.load_dataset('lukasbraach/rwth_phoenix_weather_2014', 'multisigner', streaming=True)\n",
    "it = iter(rwth_phoenix['train'])\n",
    "first = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "vmap(encode, in_dims=0, ...)(<inputs>): Got in_dim=0 for an input but the input is of type <class 'range'>. We cannot vmap over non-Tensor arguments, please use None as the respective in_dim",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m model \u001B[38;5;241m=\u001B[39m SignLanguageNet()\n\u001B[1;32m      7\u001B[0m src_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrand(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m224\u001B[39m, \u001B[38;5;241m224\u001B[39m)\n\u001B[0;32m----> 8\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m out\u001B[38;5;241m.\u001B[39mshape\n",
      "File \u001B[0;32m/usr/local/Caskroom/mambaforge/base/envs/master_thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/usr/local/Caskroom/mambaforge/base/envs/master_thesis/lib/python3.10/site-packages/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py:509\u001B[0m, in \u001B[0;36mSpeechEncoderDecoderModel.forward\u001B[0;34m(self, inputs, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, input_values, input_features, return_dict, **kwargs)\u001B[0m\n\u001B[1;32m    506\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    507\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou have to specify either input_values or input_features\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 509\u001B[0m     encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    510\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    511\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    512\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    513\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    514\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    515\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs_encoder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    516\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    517\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(encoder_outputs, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    518\u001B[0m     encoder_outputs \u001B[38;5;241m=\u001B[39m BaseModelOutput(\u001B[38;5;241m*\u001B[39mencoder_outputs)\n",
      "File \u001B[0;32m/usr/local/Caskroom/mambaforge/base/envs/master_thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Documents/PycharmProjects/master_thesis_dev/src/models/components/spatiotemporal_encoder.py:63\u001B[0m, in \u001B[0;36mSpatiotemporalEncoder.forward\u001B[0;34m(self, pixel_values, attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, pixel_values: Optional[torch\u001B[38;5;241m.\u001B[39mTensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, attention_mask: Optional[torch\u001B[38;5;241m.\u001B[39mTensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     61\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 63\u001B[0m         X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_batched_spatial_encode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batched_pos_encode(X)\n\u001B[1;32m     66\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtemporal_encoder(X, mask\u001B[38;5;241m=\u001B[39mattention_mask)\n",
      "File \u001B[0;32m~/Documents/PycharmProjects/master_thesis_dev/src/models/components/spatiotemporal_encoder.py:55\u001B[0m, in \u001B[0;36mSpatiotemporalEncoder._batched_spatial_encode\u001B[0;34m(self, pixel_values, attention_mask)\u001B[0m\n\u001B[1;32m     52\u001B[0m     mask \u001B[38;5;241m=\u001B[39m attention_mask[i] \u001B[38;5;28;01mif\u001B[39;00m (attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspatial_encoder(pixel_values\u001B[38;5;241m=\u001B[39mpixel_values[i], bool_masked_pos\u001B[38;5;241m=\u001B[39mmask)\u001B[38;5;241m.\u001B[39mpooler_output\n\u001B[0;32m---> 55\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mencode\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindices\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/Caskroom/mambaforge/base/envs/master_thesis/lib/python3.10/site-packages/torch/_functorch/vmap.py:426\u001B[0m, in \u001B[0;36mvmap.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    424\u001B[0m lazy_load_decompositions()\n\u001B[1;32m    425\u001B[0m _check_out_dims_is_int_or_int_pytree(out_dims, func)\n\u001B[0;32m--> 426\u001B[0m batch_size, flat_in_dims, flat_args, args_spec \u001B[38;5;241m=\u001B[39m \u001B[43m_process_batched_inputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43min_dims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    428\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunk_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    429\u001B[0m     chunks_flat_args \u001B[38;5;241m=\u001B[39m _get_chunked_inputs(flat_args, flat_in_dims, batch_size, chunk_size)\n",
      "File \u001B[0;32m/usr/local/Caskroom/mambaforge/base/envs/master_thesis/lib/python3.10/site-packages/torch/_functorch/vmap.py:105\u001B[0m, in \u001B[0;36m_process_batched_inputs\u001B[0;34m(in_dims, args, func)\u001B[0m\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    101\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvmap(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_get_name(func)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, in_dims=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00min_dims\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, ...)(<inputs>): \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    102\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGot in_dim=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00min_dim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for an input but in_dim must be either \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    103\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124man integer dimension or None.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(in_dim, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arg, Tensor):\n\u001B[0;32m--> 105\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    106\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvmap(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_get_name(func)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, in_dims=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00min_dims\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, ...)(<inputs>): \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    107\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGot in_dim=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00min_dim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for an input but the input is of type \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    108\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(arg)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. We cannot vmap over non-Tensor arguments, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    109\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplease use None as the respective in_dim\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m in_dim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m (in_dim \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m-\u001B[39marg\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01mor\u001B[39;00m in_dim \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m arg\u001B[38;5;241m.\u001B[39mdim()):\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    112\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvmap(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_get_name(func)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, in_dims=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00min_dims\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, ...)(<inputs>): \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    113\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGot in_dim=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00min_dim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for some input, but that input is a Tensor \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    114\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mof dimensionality \u001B[39m\u001B[38;5;132;01m{\u001B[39;00marg\u001B[38;5;241m.\u001B[39mdim()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m so expected in_dim to satisfy \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    115\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;132;01m{\u001B[39;00marg\u001B[38;5;241m.\u001B[39mdim()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m <= in_dim < \u001B[39m\u001B[38;5;132;01m{\u001B[39;00marg\u001B[38;5;241m.\u001B[39mdim()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: vmap(encode, in_dims=0, ...)(<inputs>): Got in_dim=0 for an input but the input is of type <class 'range'>. We cannot vmap over non-Tensor arguments, please use None as the respective in_dim"
     ]
    }
   ],
   "source": [
    "from src.models.components.sign_language_net import SignLanguageNet\n",
    "import torch\n",
    "\n",
    "\n",
    "model = SignLanguageNet()\n",
    "\n",
    "src_tensor = torch.rand(1, 1, 3, 224, 224)\n",
    "out = model(src_tensor)\n",
    "\n",
    "out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T16:46:13.278228Z",
     "start_time": "2023-12-05T16:46:01.484476Z"
    }
   },
   "id": "c5769d1b7e39dc4a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
