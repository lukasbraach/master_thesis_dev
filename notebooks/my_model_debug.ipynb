{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "import src.models.components.spatiotemporal_encoder\n",
    "import src.models.components.feature_extractor_dinov2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-12T10:32:59.476024Z",
     "start_time": "2023-12-12T10:32:59.458039Z"
    }
   },
   "id": "3c49661276d5a724"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-12T09:14:25.072066Z",
     "start_time": "2023-12-12T09:14:18.949845Z"
    }
   },
   "outputs": [],
   "source": [
    "rwth_phoenix = datasets.load_dataset('lukasbraach/rwth_phoenix_weather_2014', 'multisigner', streaming=True)\n",
    "it = iter(rwth_phoenix['train'])\n",
    "first = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_values': tensor([[[ 1.6742e+00, -2.8180e+00,  3.3536e+00, -2.4798e+00,  6.3762e-01,\n          -1.9210e-01,  8.8408e-01, -1.6287e+00,  6.1000e-01,  3.2373e+00,\n          -1.5032e+00, -1.5711e+00,  9.0096e-01, -1.0102e+00,  1.1974e+00,\n          -6.0821e-01, -1.3046e+00, -9.9512e-01, -7.1871e-01,  8.1806e-01,\n          -1.0132e-01,  1.6959e+00, -7.8999e-01,  3.9768e-01, -6.8739e-02,\n          -3.5846e-01, -2.2266e+00,  8.4898e-01, -6.5053e-02,  1.6005e+00,\n           7.0759e-01, -8.1556e-01, -3.5614e-02, -2.5874e+00,  5.7716e-01,\n          -3.0892e+00, -1.2169e+00, -6.6028e+00,  1.2053e-01,  1.0421e+00,\n          -1.6172e+00, -1.3854e+00, -1.3462e+00, -2.6993e+00,  3.8978e+00,\n          -3.5246e-01, -1.8882e+00, -4.4828e-01,  7.0859e-01, -3.2677e+00,\n          -3.7407e+00, -1.4602e+00,  1.3550e-01,  2.1562e+00, -1.3689e+00,\n           1.1337e+00, -2.7803e+00,  1.6022e+00,  9.7786e-01,  1.1882e+00,\n           8.6958e-01,  1.5205e+00, -7.5669e-03, -7.5505e-01, -1.7301e+00,\n          -2.4292e-01, -1.4429e-01, -4.8822e-01,  7.1377e-02, -5.2985e+00,\n           1.6328e-01, -1.3431e+00, -3.7522e+00,  8.5078e-01,  2.6858e-01,\n           5.7406e-01, -2.9601e-01,  2.2745e+00, -3.2169e+00, -9.8269e-01,\n           7.8310e-02,  1.4775e+00,  7.4602e-01,  2.4656e-01, -1.9187e+00,\n          -7.1472e-01, -2.8207e+00,  1.3810e+00,  2.1429e+00, -4.8694e-01,\n          -2.6467e+00, -3.2054e+00,  1.5084e+00, -2.1419e+00,  1.5241e+00,\n           3.6423e+00, -2.0155e+00,  1.4840e+00,  5.0826e-01, -1.0982e+00,\n          -2.5303e-02,  2.7965e+00, -6.1346e-01,  1.1838e+00,  1.1624e+00,\n           3.8446e+00,  9.8200e-01,  2.9763e+00, -3.8299e-01, -1.4592e+00,\n           4.7095e-01,  3.2072e-03,  1.3887e+00,  7.0739e-01, -2.8927e-01,\n           9.1069e-01, -6.0371e-01, -8.8136e-01, -9.1318e-01, -6.4060e-01,\n           7.1094e-01,  2.1865e-01, -3.5748e+00,  6.7116e-01,  3.9702e-01,\n          -1.4219e+00,  2.9834e+00,  1.3764e+00,  7.5671e-01,  1.9077e+00,\n          -7.6472e-01,  1.6360e+00, -6.6437e-01, -2.1107e+00, -2.9922e+00,\n           1.6698e+00, -2.3954e+00,  1.0984e-01,  4.6412e-01,  2.3731e+00,\n          -3.3430e-01, -1.6220e+00, -3.4628e-01, -5.3934e-01,  2.1773e+00,\n           6.5547e-01,  2.0675e+00,  4.3355e-02, -1.0211e+00, -2.3811e+00,\n          -9.8896e-01,  2.1802e-01, -1.9305e+00,  1.2638e+00,  1.2803e+00,\n           1.0437e+00,  1.2440e+00,  1.2431e+00, -2.8747e-01,  1.3254e+00,\n          -3.2498e+00, -5.2462e-01,  1.8593e+00, -1.7932e+00,  2.9902e-01,\n           2.5942e+00, -9.8833e-01, -2.2017e+00,  1.8137e+00,  2.3546e+00,\n           6.4141e-01, -3.3375e-01, -7.1414e-01,  5.6252e-02, -4.4311e+00,\n           6.0517e-02, -1.7898e+00,  1.5353e-01, -8.1077e-01, -8.9949e-01,\n           6.4306e-01,  6.8106e-01, -8.3384e-01,  5.1565e-02,  1.7438e+00,\n          -2.5255e+00, -2.2712e+00,  1.0668e+00,  2.0918e+00, -1.3600e+00,\n          -2.0501e+00, -3.1681e+00,  9.7735e-01,  1.9377e+00, -7.4985e-01,\n           3.4012e+00,  7.0781e-01, -2.2345e+00, -1.1709e+00, -3.8216e+00,\n          -1.5908e+00, -2.5791e+00,  1.5567e-01,  3.2324e-01, -1.1096e+00,\n           3.3369e+00, -1.0759e+00,  2.1380e+00,  4.5934e-01, -2.1277e+00,\n          -1.8057e+00, -6.4379e-01, -2.7464e+00, -1.6667e-01, -3.8065e-01,\n          -1.2917e+00, -6.0087e+00,  3.3323e-01, -1.7954e+00,  1.6225e+00,\n           3.9082e-01,  9.9918e-01,  6.5184e-01,  6.7487e-01, -6.2243e-01,\n          -1.8811e+00, -2.7683e+00,  1.5623e+00,  3.2589e+00, -3.2712e-01,\n           3.7725e+00, -8.7838e-01,  6.4092e-01,  2.6133e-01,  1.4955e+00,\n           5.5472e-01, -5.4050e-01,  1.4105e+00,  8.6670e-01,  1.5147e-01,\n          -5.1713e-01,  1.2708e+00, -6.8574e-01,  5.4553e-01,  5.4974e-02,\n          -8.7679e-01, -5.8638e-01,  1.2685e-01, -1.3286e+00,  1.1604e+00,\n           1.6962e+00,  8.9715e-01,  1.4053e+00, -2.9352e-03,  2.2648e+00,\n          -1.0802e+00,  1.6287e+00,  3.9721e-01,  6.1562e-01,  1.7587e+00,\n           2.3499e+00,  1.1068e-01,  1.5630e+00,  3.6180e-01,  1.0789e+00,\n          -9.2225e-01,  4.1518e-01, -1.0094e-01, -6.5195e+00, -2.1924e+00,\n          -2.5532e-01, -4.9417e-01, -3.3279e-01,  3.5389e+00, -7.3780e-01,\n           4.9645e+00, -6.3547e-01, -1.6899e-01, -2.0353e+00, -4.9625e-01,\n          -2.4361e-01,  2.6246e+00, -2.4380e-02,  4.1121e-01,  1.8815e+00,\n           1.9223e-01, -4.5450e-01, -6.3546e-01, -1.2825e+00, -2.9543e+00,\n           2.5818e-01, -1.8619e+00, -1.1454e-01,  1.9569e-01,  1.8428e+00,\n           1.4494e-01,  5.6710e-01,  3.1598e-01, -5.5010e-01, -2.3352e+00,\n           2.3257e+00, -4.9853e-01, -5.8658e-01,  1.5635e+00,  3.5276e+00,\n           8.3861e-01, -4.7721e-01,  4.4400e-01,  6.6067e-01,  8.5080e-01,\n          -3.2051e-01,  1.0862e+00, -3.1663e-01,  2.8771e-01,  2.5141e+00,\n           1.5850e-01, -8.2361e-01, -1.4159e+00, -1.5517e+00,  1.7630e+00,\n          -2.3686e+00, -1.7374e+00, -6.3047e-01,  2.5467e+00, -1.8746e+00,\n           3.4027e+00,  1.8671e+00,  6.7496e-01, -4.5316e-01,  9.2080e-01,\n          -2.5930e+00,  1.4793e+00,  2.9628e-01,  2.6592e+00,  2.2412e+00,\n           4.2078e-01, -8.3296e-01, -1.5015e+00, -7.1041e-01, -9.7781e-01,\n          -1.8896e+00,  1.0626e+00,  1.5641e+00,  6.2836e-01,  9.1880e-02,\n           1.9737e+00,  1.1700e+00, -9.8414e-01,  2.0979e+00, -1.0153e+00,\n          -1.3301e+00,  2.0531e+00, -2.9572e+00,  2.8180e+00,  2.7918e+00,\n          -1.5951e+00, -4.4248e-01, -1.5157e-01, -1.9353e-01,  1.1336e+00,\n           2.5603e+00, -2.2229e+00,  2.2421e+00, -1.8583e+00, -1.6482e-01,\n          -1.5694e+00, -1.4456e+00, -7.3404e-01, -4.6215e-01,  9.5709e-01,\n          -1.1095e+00,  4.2503e-01, -2.8175e+00, -5.1727e-01,  1.5469e+00,\n          -1.0772e+00, -2.4265e+00,  3.1076e-01, -5.2019e-01,  9.6893e-01,\n           7.6310e-01,  2.6744e+00, -7.1888e-01, -2.2668e+00,  4.5613e-01,\n          -1.5538e+00, -1.7379e+00,  3.7688e-01, -9.8571e-01, -8.3483e-02,\n           2.2245e+00, -1.5319e+00, -1.4947e+00,  2.0891e+00,  5.3630e+00,\n          -7.8571e-04,  6.3392e-01,  7.9457e-01,  8.3210e-01, -1.1200e-01,\n           2.1911e+00, -2.3314e+00, -3.4581e-01, -3.3367e-01,  4.3330e-01,\n          -1.6320e+00, -7.8430e-01, -1.9619e+00,  4.0948e+00, -1.0505e+00,\n          -1.5094e+00, -2.7234e+00, -4.4076e-01,  1.7240e-01,  1.4790e+00,\n           4.0902e+00,  3.7194e-01,  2.5706e+00,  6.6458e-01, -3.5907e-01,\n          -7.6393e-01,  1.0162e+00,  1.1589e+00, -1.7048e+00, -3.0000e+00,\n           4.8628e-01,  1.4133e-01,  2.2131e+00,  7.0168e-01, -9.8999e-03,\n          -1.5970e+00,  2.4749e+00,  8.3020e-01,  6.4554e-01, -1.8550e+00,\n          -3.9918e-01,  5.9080e-02,  1.9815e-01, -3.4079e-02, -6.7196e-01,\n           1.1638e+00,  1.0820e+00,  3.8805e+00, -1.9869e+00,  1.0254e+00,\n           2.3704e-01,  2.5091e+00, -1.2580e+00, -8.8545e-01, -6.2080e-01,\n           3.6141e+00,  2.2588e+00,  3.5943e-02, -3.2639e-01,  2.1925e+00,\n          -9.6358e-01,  4.3979e-01,  2.0770e-01,  7.3290e-01, -1.1052e+00,\n           1.9818e+00,  2.1925e+00,  5.0640e-01, -1.0287e+00, -3.8091e+00,\n           1.7861e+00,  5.8372e-01, -2.8096e+00,  4.0412e+00,  3.8540e-01,\n           4.6144e-01, -5.4093e-01, -1.0162e+00,  2.1306e-01,  5.9770e-02,\n          -1.3092e+00,  1.3186e+00,  4.7024e-01, -9.3985e-01,  2.2133e+00,\n          -9.1383e-02, -1.0173e+00,  3.1064e-01,  8.6155e-01, -3.7491e-01,\n          -2.1647e+00, -2.4496e+00,  2.2756e+00,  2.2356e+00,  4.7958e-01,\n          -1.2246e-01, -1.5732e+00,  3.8506e-01,  1.5336e+00, -1.3104e+00,\n           1.4501e+00, -1.8227e+00,  1.0663e+00, -5.1081e-01,  7.0070e-01,\n          -3.7990e+00,  3.8383e-01,  2.4415e+00, -1.1941e+00, -2.7085e+00,\n           3.9045e-01,  4.5925e-01,  1.8956e+00, -4.0527e-01, -2.7951e-01,\n           1.9065e+00,  2.4061e-01, -6.7943e-01, -2.0480e-01,  5.8636e-01,\n           1.2774e+00,  1.7001e+00,  4.5025e-01,  5.4940e-01,  3.1389e-01,\n           2.1769e+00, -1.9060e+00,  4.1147e-01,  6.7440e-01,  3.2497e+00,\n           7.8225e-01, -2.0651e+00,  2.1555e-01, -3.2778e-01,  2.8076e-01,\n          -3.2063e-01, -7.8836e+00,  4.6015e-01, -1.7482e+00,  2.3268e+00,\n          -1.4792e+00, -1.4433e+00,  4.3109e-02, -1.5342e+00,  6.9253e-01,\n          -5.3870e+00, -7.6599e-01,  1.8257e+00,  1.2097e+00,  6.1954e-01,\n           1.6241e+00, -1.4043e+00,  8.6777e-03, -1.1361e-01, -3.4433e+00,\n          -2.9439e+00, -1.6296e+00, -6.4847e-01,  7.1223e-01,  2.4010e+00,\n          -1.0298e+00,  1.7156e-01, -1.2716e-01,  4.6956e-01, -3.9454e-01,\n           4.0250e+00,  3.5882e+00, -6.0192e-01, -1.0775e+00,  8.9824e-01,\n           9.9694e-01,  1.0581e+00, -1.8718e-02, -7.6443e+00, -2.1178e+00,\n          -8.2502e-01,  6.8924e-01, -1.2482e-01,  4.5393e-01,  3.6088e+00,\n           1.0979e+00,  3.0173e+00, -2.6565e-01, -1.9616e-01, -2.9782e+00,\n           1.9388e+00,  3.8041e+00,  9.4671e-01,  2.3838e+00,  1.2908e+00,\n          -1.2436e+00,  5.2485e-02, -8.3436e-01, -1.8313e+00,  8.0996e-01,\n           8.5673e-01, -1.7887e-01,  3.1999e-01,  2.0716e+00, -1.5216e+00,\n          -3.4592e+00, -1.1182e+00,  7.9093e-01,  2.3267e+00,  4.8162e-01,\n           3.1826e+00, -1.0497e+00, -6.4404e-01,  1.2271e-01, -2.2188e-02,\n          -5.3983e-01, -4.6479e-01, -2.1531e-01,  1.0228e+00,  1.0161e+00,\n          -3.5881e+00,  8.4269e-02,  8.1975e-02, -1.3941e+00,  9.2967e-01,\n          -1.5456e+00, -8.2314e-01, -1.3352e+00, -2.5011e+00, -1.6778e+00,\n          -9.5341e-01,  2.5409e+00, -1.2370e+00, -1.2654e+00,  9.4512e-01,\n          -1.4655e+00, -2.1095e+00, -6.9713e-01, -6.7284e-01, -1.7598e+00,\n          -3.2579e-02,  6.2767e-01, -1.7104e+00, -1.4928e+00,  1.4733e+00,\n          -3.3977e+00, -1.0662e+00,  1.8331e+00, -1.1186e+00, -1.7772e+00,\n           2.0330e+00, -5.4909e-02, -2.3944e+00, -7.5804e-01,  1.5468e+00,\n           1.5779e+00, -1.1721e-02, -6.7330e-01, -9.6036e-01,  2.4287e+00,\n           3.1785e-01,  2.1579e-01, -1.0158e+00, -3.1744e+00,  1.3312e+00,\n           2.2107e+00,  1.3584e+00, -2.2872e+00,  3.1125e+00, -1.5999e+00,\n          -5.3192e-01, -6.5388e+00,  1.9634e+00, -2.4785e-01, -9.4887e-01,\n           1.0235e+00, -7.4957e-01,  4.2405e-01, -9.1170e-01,  8.0396e-01,\n          -4.9185e-02,  9.2284e-01, -2.2703e+00,  2.8357e-01, -1.6342e+00,\n           1.0168e+00, -9.7109e-01, -1.8489e+00, -9.2848e-01, -1.3073e+00,\n          -1.3484e+00, -1.4809e-02,  1.1323e+00,  2.4918e-01, -5.4076e-01,\n           1.3308e+00,  9.4634e-01, -3.1458e-01, -3.3757e+00, -4.8300e-01,\n           1.3953e+00,  3.0084e+00,  6.7994e-01, -7.7297e-01,  3.1605e-01,\n          -3.3911e-01,  2.6218e+00, -6.3489e-01, -5.9250e-01, -3.3989e+00,\n          -1.7378e+00, -8.1808e-01, -2.4091e+00, -3.3011e-03, -1.2918e+00,\n          -1.4816e+00,  3.9623e+00,  5.3069e-01, -4.8414e+00, -6.5035e-01,\n           3.9027e-01,  4.4074e-01,  2.2251e-01, -6.3040e+00, -1.9919e+00,\n           2.1448e+00,  1.4801e+00, -1.1885e+00,  7.3358e-01, -1.3411e+00,\n          -1.2986e+00, -1.5029e+00, -1.6299e-01, -1.1092e+00,  6.2362e-01,\n          -8.3645e-01, -8.5957e-01, -1.7453e+00, -5.6075e-01, -4.4038e-01,\n          -1.8333e+00, -1.1218e+00,  1.3091e+00, -2.4763e+00,  1.7926e+00,\n          -9.3165e-01, -5.0432e-02, -9.3774e-01,  1.1221e+00, -1.8368e-01,\n           2.7939e+00, -1.5480e+00,  3.1619e+00,  2.4892e+00, -1.2429e+00,\n          -1.0410e+00,  1.9269e+00, -1.3653e-01, -3.9034e-01, -3.6736e-02,\n          -1.5736e+00, -1.0607e-01, -4.4107e-01,  3.7604e-01,  5.2254e-01,\n          -1.1437e+00,  7.9934e-01,  1.2293e-01,  1.2064e+00,  8.5064e-02,\n          -9.8890e-01,  1.4010e+00,  1.4249e+00,  9.9980e-01, -1.1713e+00,\n          -6.1076e-01,  3.9983e-01, -1.0821e+00]]])}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(src.models.components.spatiotemporal_encoder)\n",
    "reload(src.models.components.feature_extractor_dinov2)\n",
    "from src.models.components.spatiotemporal_encoder import SpatiotemporalEncoder\n",
    "from src.models.components.feature_extractor_dinov2 import SignLanguageFeatureExtractor\n",
    "\n",
    "feature_extractor = SignLanguageFeatureExtractor()\n",
    "model = SpatiotemporalEncoder()\n",
    "\n",
    "src_tensor = np.random.rand(1, 1, 3, 224, 224)\n",
    "batched_feature = feature_extractor(src_tensor, sampling_rate=25, return_tensors='pt')\n",
    "\n",
    "batched_feature"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-12T10:43:48.006482Z",
     "start_time": "2023-12-12T10:43:46.647636Z"
    }
   },
   "id": "75911605c234a03c"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m model \u001B[38;5;241m=\u001B[39m SignLanguageNet()\n\u001B[1;32m      7\u001B[0m src_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrand(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m224\u001B[39m, \u001B[38;5;241m224\u001B[39m)\n\u001B[0;32m----> 8\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m out\u001B[38;5;241m.\u001B[39mshape\n",
      "File \u001B[0;32m/usr/local/Caskroom/mambaforge/base/envs/master_thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/usr/local/Caskroom/mambaforge/base/envs/master_thesis/lib/python3.10/site-packages/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py:509\u001B[0m, in \u001B[0;36mSpeechEncoderDecoderModel.forward\u001B[0;34m(self, inputs, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, input_values, input_features, return_dict, **kwargs)\u001B[0m\n\u001B[1;32m    506\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    507\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou have to specify either input_values or input_features\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 509\u001B[0m     encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    510\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    511\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    512\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    513\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    514\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    515\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs_encoder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    516\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    517\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(encoder_outputs, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    518\u001B[0m     encoder_outputs \u001B[38;5;241m=\u001B[39m BaseModelOutput(\u001B[38;5;241m*\u001B[39mencoder_outputs)\n",
      "File \u001B[0;32m/usr/local/Caskroom/mambaforge/base/envs/master_thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Documents/PycharmProjects/master_thesis_dev/src/models/components/spatiotemporal_encoder.py:62\u001B[0m, in \u001B[0;36mSpatiotemporalEncoder.forward\u001B[0;34m(self, pixel_values, attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, pixel_values: Optional[torch\u001B[38;5;241m.\u001B[39mTensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, attention_mask: Optional[torch\u001B[38;5;241m.\u001B[39mTensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     60\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 62\u001B[0m         X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_batched_spatial_encode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     64\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batched_pos_encode(X)\n\u001B[1;32m     65\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtemporal_encoder(X, mask\u001B[38;5;241m=\u001B[39mattention_mask)\n",
      "File \u001B[0;32m~/Documents/PycharmProjects/master_thesis_dev/src/models/components/spatiotemporal_encoder.py:54\u001B[0m, in \u001B[0;36mSpatiotemporalEncoder._batched_spatial_encode\u001B[0;34m(self, pixel_values, attention_mask)\u001B[0m\n\u001B[1;32m     51\u001B[0m     mask \u001B[38;5;241m=\u001B[39m attention_mask[i] \u001B[38;5;28;01mif\u001B[39;00m (attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspatial_encoder(pixel_values\u001B[38;5;241m=\u001B[39mpixel_values[i], bool_masked_pos\u001B[38;5;241m=\u001B[39mmask)\u001B[38;5;241m.\u001B[39mpooler_output\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mvmap(encode)(\u001B[43mindices\u001B[49m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'indices' is not defined"
     ]
    }
   ],
   "source": [
    "from src.models.components.sign_language_net import SignLanguageNet\n",
    "import torch\n",
    "\n",
    "model = SignLanguageNet()\n",
    "\n",
    "src_tensor = torch.rand(1, 1, 3, 224, 224)\n",
    "out = model(src_tensor)\n",
    "\n",
    "out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-12T09:10:59.675500Z",
     "start_time": "2023-12-12T09:10:55.904475Z"
    }
   },
   "id": "c5769d1b7e39dc4a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
