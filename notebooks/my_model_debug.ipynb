{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "import src.models.components.feature_extractor_dinov2\n",
    "import src.models.components.spatiotemporal_encoder"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T12:28:49.798515Z",
     "start_time": "2023-12-19T12:28:44.040071Z"
    }
   },
   "id": "3c49661276d5a724"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-19T12:28:54.883121Z",
     "start_time": "2023-12-19T12:28:49.798780Z"
    }
   },
   "outputs": [],
   "source": [
    "rwth_phoenix = datasets.load_dataset('lukasbraach/rwth_phoenix_weather_2014', 'multisigner', streaming=True)\n",
    "it = iter(rwth_phoenix['train'])\n",
    "first = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_values: torch.Size([3, 32, 768])\n",
      "attention_mask: tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "reload(src.models.components.feature_extractor_dinov2)\n",
    "from src.models.components.feature_extractor_dinov2 import SignLanguageFeatureExtractor\n",
    "\n",
    "feature_extractor = SignLanguageFeatureExtractor()\n",
    "\n",
    "src_tensor = [\n",
    "    np.random.rand(3, 3, 224, 224),\n",
    "    np.random.rand(1, 3, 224, 224),\n",
    "    np.random.rand(6, 3, 224, 224),\n",
    "]\n",
    "batched_feature = feature_extractor(src_tensor, sampling_rate=25, pad_to_multiple_of=32, return_tensors=\"pt\")\n",
    "\n",
    "input_values = batched_feature.input_values\n",
    "attention_mask = batched_feature.attention_mask\n",
    "\n",
    "print(\"input_values:\", input_values.shape)\n",
    "print(\"attention_mask:\", attention_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T12:38:06.893799Z",
     "start_time": "2023-12-19T12:38:03.246472Z"
    }
   },
   "id": "75911605c234a03c"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "Wav2Vec2BaseModelOutput(last_hidden_state=tensor([[[-1.3873,  1.4464,  0.6032,  ...,  0.5986,  0.6305, -1.9885],\n         [-0.7752,  1.3980,  1.1602,  ..., -0.3088, -0.1042, -1.0873],\n         [-0.9764,  1.5781,  1.1313,  ...,  0.4113, -0.3345, -1.8981],\n         ...,\n         [-0.7431,  1.3568, -0.0080,  ...,  0.9427,  0.2413, -0.6780],\n         [-0.7913,  1.1805, -0.2928,  ...,  1.3326,  0.5124,  0.2337],\n         [-0.4308,  1.3446,  0.4378,  ...,  1.2666,  0.1246, -0.1829]],\n\n        [[-1.4415, -1.2161,  0.4831,  ...,  0.9675,  0.6535,  1.0248],\n         [ 0.5653,  0.7371,  2.0587,  ..., -0.4363,  1.0376,  1.0097],\n         [ 0.5841,  0.9852, -0.4640,  ...,  0.0693, -0.6144,  0.6243],\n         ...,\n         [-0.9704, -1.0217,  0.0294,  ...,  0.8167,  0.6037, -0.2885],\n         [ 0.7069,  1.1435, -1.9218,  ...,  0.5191, -1.1076, -1.9945],\n         [ 1.0791,  1.1473, -0.2925,  ..., -0.2680, -0.1915, -1.0578]],\n\n        [[ 0.1232,  0.8341,  1.4454,  ..., -0.0923,  0.4459,  1.5432],\n         [ 0.5811, -0.2335, -0.2183,  ...,  0.5733, -1.1689,  1.4002],\n         [ 0.6073,  0.5184, -0.1256,  ...,  0.9550, -0.8775,  0.8316],\n         ...,\n         [-1.1988,  1.5262,  0.8942,  ...,  0.1660, -0.0605, -0.3783],\n         [-0.9268,  1.9542,  1.6449,  ...,  0.5432, -0.4828, -0.9117],\n         [-0.7394,  2.2386,  0.4707,  ...,  1.3458,  0.6370,  0.7113]]],\n       grad_fn=<NativeLayerNormBackward0>), extract_features=tensor([[[ 0.2791,  0.5845,  0.7604,  ...,  0.2043,  0.0096,  0.0313],\n         [ 0.2791,  0.5845,  0.7604,  ...,  0.2043,  0.0096,  0.0313],\n         [ 0.2791,  0.5845,  0.7604,  ...,  0.2043,  0.0096,  0.0313],\n         ...,\n         [-0.1518,  0.0455, -0.4614,  ...,  1.3832, -0.3708, -0.2040],\n         [-0.1518,  0.0455, -0.4614,  ...,  1.3832, -0.3708, -0.2040],\n         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n\n        [[-1.1075, -1.6544,  1.6131,  ..., -1.0413,  0.9374,  1.2043],\n         [-0.5717, -0.4852,  1.6743,  ..., -0.9813,  0.1576,  1.2005],\n         [ 1.3441,  1.5164,  0.7384,  ...,  0.9762, -1.6573,  1.3969],\n         ...,\n         [-0.5593, -0.6291, -0.8407,  ..., -0.6480,  0.6438,  0.3331],\n         [ 1.1214, -0.0039, -1.5806,  ..., -1.5416, -0.5587, -1.0215],\n         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n\n        [[-0.5717, -0.4852,  1.6743,  ..., -0.9813,  0.1576,  1.2005],\n         [ 1.3441,  1.5164,  0.7384,  ...,  0.9762, -1.6573,  1.3969],\n         [ 1.3441,  1.5164,  0.7384,  ...,  0.9762, -1.6573,  1.3969],\n         ...,\n         [ 0.2791,  0.5845,  0.7604,  ...,  0.2043,  0.0096,  0.0313],\n         [ 0.2791,  0.5845,  0.7604,  ...,  0.2043,  0.0096,  0.0313],\n         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n       grad_fn=<IndexPutBackward0>), hidden_states=None, attentions=None)"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(src.models.components.spatiotemporal_encoder)\n",
    "from src.models.components.spatiotemporal_encoder import SpatiotemporalEncoder\n",
    "from src.models.components.spatiotemporal_encoder import SpatiotemporalEncoderConfig\n",
    "\n",
    "config = SpatiotemporalEncoderConfig()\n",
    "\n",
    "model = SpatiotemporalEncoder(config)\n",
    "encoded = model(input_values, attention_mask)\n",
    "\n",
    "encoded"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T15:23:35.329057Z",
     "start_time": "2023-12-19T15:23:34.668954Z"
    }
   },
   "id": "434c96cc6a7db9ed"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "SignLanguageNet(\n  (encoder): SpatiotemporalEncoder(\n    (feature_extractor): SpatialFeatureEncoder()\n    (feature_projection): SpatiotemporalFeatureProjection(\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): Wav2Vec2Encoder(\n      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n        (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n        (padding): Wav2Vec2SamePadLayer()\n        (activation): GELUActivation()\n      )\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (layers): ModuleList(\n        (0-5): 6 x Wav2Vec2EncoderLayer(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (decoder): LanguageDecoder(\n    (model): Speech2Text2DecoderWrapper(\n      (decoder): Speech2Text2Decoder(\n        (embed_tokens): Embedding(10000, 1024, padding_idx=1)\n        (embed_positions): Speech2Text2SinusoidalPositionalEmbedding()\n        (layers): ModuleList(\n          (0-5): 6 x Speech2Text2DecoderLayer(\n            (self_attn): Speech2Text2Attention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): Speech2Text2Attention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=1024, out_features=2048, bias=True)\n            (fc2): Linear(in_features=2048, out_features=1024, bias=True)\n            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (lm_head): Linear(in_features=1024, out_features=10000, bias=False)\n  )\n  (enc_to_dec_proj): Linear(in_features=768, out_features=1024, bias=True)\n)"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(src.models.components.sign_language_net)\n",
    "from src.models.components.sign_language_net import SignLanguageNet\n",
    "\n",
    "\n",
    "model = SignLanguageNet()\n",
    "\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T15:24:08.994342Z",
     "start_time": "2023-12-19T15:24:07.402946Z"
    }
   },
   "id": "c5769d1b7e39dc4a"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2, 2],\n        [2, 2],\n        [2, 2]])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.generate(inputs=input_values, attention_mask=attention_mask, max_length=100)\n",
    "\n",
    "out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T15:24:05.304055Z",
     "start_time": "2023-12-19T15:24:05.202913Z"
    }
   },
   "id": "136d75181c81dc5a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
