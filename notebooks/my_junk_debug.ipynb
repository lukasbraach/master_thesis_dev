{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-28T15:54:50.645966Z",
     "start_time": "2023-11-28T15:54:48.952773Z"
    }
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Resolving data files:   0%|          | 0/103 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c1e144c5a314161855a00b2c5ebe355"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rwth_phoenix = datasets.load_dataset('lukasbraach/rwth_phoenix_weather_2014', 'multisigner', streaming=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T16:18:15.302680Z",
     "start_time": "2023-11-21T16:16:35.934973Z"
    }
   },
   "id": "ea6fd5de1d7aa779"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "{'tokens': ['ABER', 'FREUEN', 'MORGEN', 'SONNE', 'SELTEN', 'REGEN'],\n 'frames': [<PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>,\n  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=210x260>]}"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(rwth_phoenix['test'])\n",
    "first = next(it)\n",
    "\n",
    "first"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T16:35:51.248042Z",
     "start_time": "2023-11-21T16:35:35.836542Z"
    }
   },
   "id": "9dbc1fbfc36d6f95"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "132"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first['frames'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:09:58.945810Z",
     "start_time": "2023-11-21T17:09:58.878826Z"
    }
   },
   "id": "f0d557683707d8f8"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m reload(\u001B[43msrc\u001B[49m\u001B[38;5;241m.\u001B[39mmodels\u001B[38;5;241m.\u001B[39mcomponents\u001B[38;5;241m.\u001B[39mfeature_extractor_dinov2)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomponents\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_extractor_dinov2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SignLanguageFeatureExtractor\n\u001B[1;32m      4\u001B[0m feature_extractor \u001B[38;5;241m=\u001B[39m SignLanguageFeatureExtractor()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'src' is not defined"
     ]
    }
   ],
   "source": [
    "reload(src.models.components.feature_extractor_dinov2)\n",
    "from src.models.components.feature_extractor_dinov2 import SignLanguageFeatureExtractor\n",
    "\n",
    "feature_extractor = SignLanguageFeatureExtractor()\n",
    "feature = feature_extractor(first['frames'], sampling_rate=25, return_attention_mask=True, return_tensors='pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T15:55:26.056488Z",
     "start_time": "2023-11-28T15:55:26.048387Z"
    }
   },
   "id": "fe2027d2bcc4baa0"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 132, 768])"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature['input_values'].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T17:10:54.025967Z",
     "start_time": "2023-11-21T17:10:53.959265Z"
    }
   },
   "id": "292f10f6df5e2f9a"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from src.models.components.sign_language_net import SignLanguageNet\n",
    "\n",
    "model = SignLanguageNet()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T16:24:27.082036Z",
     "start_time": "2023-11-28T16:24:21.142444Z"
    }
   },
   "id": "7bda0a9b5abb169d"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You have to specify either decoder_input_ids or decoder_inputs_embeds",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      3\u001B[0m src_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrand(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m224\u001B[39m, \u001B[38;5;241m224\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m out\u001B[38;5;241m.\u001B[39mshape\n",
      "File \u001B[0;32m/usr/local/Caskroom/mambaforge/base/envs/master_thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/usr/local/Caskroom/mambaforge/base/envs/master_thesis/lib/python3.10/site-packages/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py:543\u001B[0m, in \u001B[0;36mSpeechEncoderDecoderModel.forward\u001B[0;34m(self, inputs, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, input_values, input_features, return_dict, **kwargs)\u001B[0m\n\u001B[1;32m    538\u001B[0m     decoder_input_ids \u001B[38;5;241m=\u001B[39m shift_tokens_right(\n\u001B[1;32m    539\u001B[0m         labels, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpad_token_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mdecoder_start_token_id\n\u001B[1;32m    540\u001B[0m     )\n\u001B[1;32m    542\u001B[0m \u001B[38;5;66;03m# Decode\u001B[39;00m\n\u001B[0;32m--> 543\u001B[0m decoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    544\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    545\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    546\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    547\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    548\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_inputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    549\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    550\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    551\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    552\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    553\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    554\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs_decoder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    555\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    557\u001B[0m \u001B[38;5;66;03m# Compute loss independent from decoder (as some shift the logits inside them)\u001B[39;00m\n\u001B[1;32m    558\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/Caskroom/mambaforge/base/envs/master_thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/usr/local/Caskroom/mambaforge/base/envs/master_thesis/lib/python3.10/site-packages/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py:923\u001B[0m, in \u001B[0;36mSpeech2Text2ForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    920\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m    922\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[0;32m--> 923\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    924\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    925\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    926\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    927\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    928\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    929\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    930\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    931\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    932\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    933\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    934\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    935\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    936\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    938\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head(outputs[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m    940\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/Caskroom/mambaforge/base/envs/master_thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/usr/local/Caskroom/mambaforge/base/envs/master_thesis/lib/python3.10/site-packages/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py:616\u001B[0m, in \u001B[0;36mSpeech2Text2Decoder.forward\u001B[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    614\u001B[0m     input_shape \u001B[38;5;241m=\u001B[39m inputs_embeds\u001B[38;5;241m.\u001B[39msize()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    615\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 616\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou have to specify either decoder_input_ids or decoder_inputs_embeds\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    618\u001B[0m \u001B[38;5;66;03m# past_key_values_length\u001B[39;00m\n\u001B[1;32m    619\u001B[0m past_key_values_length \u001B[38;5;241m=\u001B[39m past_key_values[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_values \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n",
      "\u001B[0;31mValueError\u001B[0m: You have to specify either decoder_input_ids or decoder_inputs_embeds"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "src_tensor = torch.rand(1, 3, 224, 224)\n",
    "out = model(src_tensor)\n",
    "\n",
    "out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T16:25:26.628492Z",
     "start_time": "2023-11-28T16:25:25.385509Z"
    }
   },
   "id": "9ba8c86d96a7711"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
