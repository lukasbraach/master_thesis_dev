# @package _global_

# overfits to 3 batches

defaults:
  - default
  - override /trainer: deepspeed

trainer:
  max_epochs: 10
  overfit_batches: 5
  limit_val_batches: 0
  check_val_every_n_epoch: 1000

  accelerator: gpu
  devices: 2

model:
  optimizer:
    lr: 1e-4
  scheduler: null

data:
  num_workers: 16
  batch_size: 4
  pin_memory: True

test: False

# model ckpt and early stopping need to be disabled during overfitting
callbacks:
  model_summary:
    _target_: lightning.pytorch.callbacks.RichModelSummary
    max_depth: -1

  rich_progress_bar:
    _target_: lightning.pytorch.callbacks.RichProgressBar
